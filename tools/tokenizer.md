**Tokenizer**: Berechnet die Anzahl Tokens, Wörter und Sätze in einem Text oder einer Liste von Texten. Die Tokenisierung erfolgt mit dem Modell *gpt-3.5-turbo*. Andere LLM verwenden andere Algorithemn für die Tokenisierung und können daher zu anderen Ergebnissen führen. Die Anzahl der Tokens eines Dokuments kann als Grössenordnung für den zu erwarteten Kosten für eine Zusamenfassung verwendet werden. 


